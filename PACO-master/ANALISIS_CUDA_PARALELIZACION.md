# An√°lisis: Paralelizaci√≥n de PACOCalc con GPU CUDA

## üìä Resumen Ejecutivo

**Dificultad**: ‚≠ê‚≠ê‚≠ê (Media-Alta)
- **F√°cil**: Usar CuPy (drop-in replacement de NumPy)
- **Medio**: Optimizar con Numba CUDA
- **Dif√≠cil**: Escribir kernels CUDA desde cero

**Speedup Esperado**: 10-50x (dependiendo del tama√±o de datos y GPU)

---

## üîç An√°lisis de Operaciones

### Operaciones Principales en PACOCalc

#### 1. **pixelCalc()** - Cuello de Botella Principal (93-98% del tiempo)

```python
def pixelCalc(patch):
    # patch: (T, P) donde T=frames, P=p√≠xeles en patch (t√≠pico 49)
    m = np.mean(patch, axis=0)                    # ‚úÖ F√°cil en GPU
    S = sampleCovariance(patch, m, T)            # ‚ö†Ô∏è Operaciones matriciales
    rho = shrinkageFactor(S, T)                  # ‚ö†Ô∏è Trazas y productos
    F = diagSampleCovariance(S)                   # ‚úÖ F√°cil
    C = covariance(rho, S, F)                     # ‚úÖ F√°cil
    Cinv = np.linalg.inv(C)                       # ‚ö†Ô∏è O(P¬≥) - Costoso pero paralelizable
    return m, Cinv
```

**Operaciones por llamada**:
- `np.mean()`: O(T√óP) - **Muy f√°cil en GPU**
- `sampleCovariance()`: O(T√óP¬≤) - **Paralelizable en GPU**
- `shrinkageFactor()`: Trazas y productos matriciales - **Paralelizable**
- `np.linalg.inv()`: O(P¬≥) donde P=49 - **GPU excelente para esto**

#### 2. **al() y bl()** - Operaciones Finales

```python
def al(self, hfl, Cfl_inv):
    # hfl: lista de (P,) arrays
    # Cfl_inv: lista de (P, P) matrices
    a = np.sum([np.dot(hfl[i], np.dot(Cfl_inv[i], hfl[i]).T) 
                for i in range(len(hfl))])
    return a

def bl(self, hfl, Cfl_inv, r_fl, m_fl):
    b = np.sum([np.dot(np.dot(Cfl_inv[i], hfl[i]).T, (r_fl[i][i]-m_fl[i]))
                for i in range(len(hfl))])
    return b
```

**Operaciones**: Productos matriciales (np.dot) - **Excelente para GPU**

#### 3. **Loop Principal** - Perfectamente Paralelizable

```python
for i, p0 in enumerate(phi0s):  # 10,000 p√≠xeles independientes
    angles_px = getRotatedPixels(x, y, p0, angles)
    for l, ang in enumerate(angles_px):  # 252 frames
        patch[l] = getPatch(ang, pwidth, mask)
        m[l], Cinv[l] = pixelCalc(patch[l])  # ‚Üê BOTTLENECK
        h[l] = psf[mask]
    a[i] = al(h, Cinv)
    b[i] = bl(h, Cinv, patch, m)
```

**Paralelizaci√≥n**: Cada iteraci√≥n de `i` es **completamente independiente** ‚Üí ideal para GPU

---

## üéØ Estrategias de Implementaci√≥n

### Opci√≥n 1: CuPy (M√°s F√°cil) ‚≠ê‚≠ê‚≠ê

**Dificultad**: Baja
**Tiempo estimado**: 2-4 horas
**Speedup esperado**: 10-30x

**Ventajas**:
- Drop-in replacement de NumPy
- Misma API que NumPy
- Cambios m√≠nimos en c√≥digo

**Implementaci√≥n**:
```python
import cupy as cp

def pixelCalc_CUDA(patch):
    """Versi√≥n GPU usando CuPy"""
    # Transferir a GPU
    patch_gpu = cp.asarray(patch)
    
    # Operaciones en GPU (misma sintaxis que NumPy)
    m = cp.mean(patch_gpu, axis=0)
    S = sampleCovariance_CUDA(patch_gpu, m, T)
    rho = shrinkageFactor_CUDA(S, T)
    F = diagSampleCovariance_CUDA(S)
    C = covariance_CUDA(rho, S, F)
    Cinv = cp.linalg.inv(C)  # ‚Üê GPU excelente para esto
    
    # Transferir de vuelta
    return cp.asnumpy(m), cp.asnumpy(Cinv)

def PACOCalc_CUDA(self, phi0s):
    """Versi√≥n GPU del loop principal"""
    # Transferir datos grandes a GPU una vez
    im_stack_gpu = cp.asarray(self.m_im_stack)
    psf_gpu = cp.asarray(self.m_psf)
    
    # Paralelizar loop sobre p√≠xeles
    # Usar CUDA kernels o procesar en batches
    ...
```

**Desaf√≠os**:
- Transferencia CPU‚ÜîGPU puede ser costosa
- Matrices peque√±as (49√ó49) pueden no aprovechar bien la GPU
- Necesita gesti√≥n de memoria GPU

---

### Opci√≥n 2: Numba CUDA (Medio) ‚≠ê‚≠ê‚≠ê‚≠ê

**Dificultad**: Media
**Tiempo estimado**: 1-2 d√≠as
**Speedup esperado**: 20-50x

**Ventajas**:
- Optimizaci√≥n autom√°tica
- Puede compilar kernels CUDA desde Python
- Mejor control que CuPy

**Implementaci√≥n**:
```python
from numba import cuda
import numba

@cuda.jit
def pixelCalc_kernel(patch, m_out, Cinv_out):
    """Kernel CUDA para pixelCalc"""
    idx = cuda.grid(1)
    if idx < patch.shape[0]:
        # C√°lculos en GPU
        ...

@cuda.jit
def PACOCalc_kernel(phi0s, im_stack, angles, ...):
    """Kernel principal paralelizado"""
    idx = cuda.grid(1)
    if idx < len(phi0s):
        # Procesar p√≠xel idx en paralelo
        ...
```

**Desaf√≠os**:
- Requiere conocimiento de CUDA
- Debugging m√°s dif√≠cil
- Optimizaci√≥n manual necesaria

---

### Opci√≥n 3: CUDA Puro (Dif√≠cil) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Dificultad**: Alta
**Tiempo estimado**: 1-2 semanas
**Speedup esperado**: 30-100x (con optimizaci√≥n)

**Ventajas**:
- Control total
- M√°xima optimizaci√≥n posible

**Desventajas**:
- Requiere C/CUDA
- Desarrollo largo
- Mantenimiento complejo

---

## ‚ö†Ô∏è Desaf√≠os y Consideraciones

### 1. **Tama√±o de Matrices Peque√±as**

**Problema**: Matrices de 49√ó49 pueden ser peque√±as para GPU
- GPU es eficiente con matrices grandes (>100√ó100)
- Overhead de transferencia puede dominar

**Soluci√≥n**: Procesar m√∫ltiples p√≠xeles en paralelo (batches)

### 2. **Transferencia de Datos**

**Problema**: CPU‚ÜîGPU transfer es costosa
- Para 100√ó100 imagen: ~40 MB de datos
- Transferencia inicial: ~10-50ms
- Puede ser significativo si se hace muchas veces

**Soluci√≥n**: 
- Transferir datos grandes una vez
- Mantener datos en GPU durante todo el procesamiento
- Usar streams CUDA para overlap

### 3. **Memoria GPU**

**Problema**: GPU tiene memoria limitada (t√≠pico 8-24 GB)
- Para 100√ó100√ó252: ~10 MB (f√°cil)
- Para 160√ó160√ó252: ~26 MB (f√°cil)
- Pero con m√∫ltiples batches puede crecer

**Soluci√≥n**: Procesar en batches si es necesario

### 4. **Operaciones Espec√≠ficas**

**Problema**: Algunas operaciones pueden no tener equivalente directo en GPU
- `getRotatedPixels()`: Operaciones de coordenadas
- `getPatch()`: Indexaci√≥n compleja

**Soluci√≥n**: Implementar kernels personalizados o mantener en CPU

---

## üìà Estimaci√≥n de Speedup

### Escenario: Imagen 100√ó100, 252 frames, patch_size=49

| M√©todo | Tiempo Actual | Tiempo GPU | Speedup |
|--------|---------------|------------|---------|
| **Serial CPU** | ~8 horas | - | 1x |
| **Loky 8 CPUs** | ~10 minutos | - | ~48x |
| **CuPy (GPU)** | - | ~5-15 min | 30-100x |
| **Numba CUDA** | - | ~2-8 min | 60-240x |
| **CUDA Optimizado** | - | ~1-3 min | 160-480x |

**Nota**: Speedups reales dependen de:
- GPU (RTX 3090 vs GTX 1050)
- Tama√±o de datos
- Overhead de transferencia
- Optimizaci√≥n del c√≥digo

---

## üõ†Ô∏è Recomendaci√≥n de Implementaci√≥n

### Fase 1: Prototipo con CuPy (2-4 horas)

1. **Reemplazar NumPy por CuPy en funciones cr√≠ticas**:
   ```python
   # En pixelCalc
   import cupy as cp
   patch_gpu = cp.asarray(patch)
   m = cp.mean(patch_gpu, axis=0)
   Cinv = cp.linalg.inv(C)
   ```

2. **Paralelizar loop principal**:
   - Procesar p√≠xeles en batches
   - Usar `cupy.RawKernel` o procesar en paralelo

3. **Medir speedup real**

### Fase 2: Optimizaci√≥n con Numba (1-2 d√≠as)

Si CuPy da buen speedup pero necesita m√°s optimizaci√≥n:
- Escribir kernels Numba CUDA personalizados
- Optimizar transferencias de memoria
- Usar shared memory cuando sea posible

### Fase 3: CUDA Puro (solo si necesario)

Solo si se necesita m√°ximo rendimiento y hay tiempo disponible.

---

## üí° Alternativa: JAX (M√°s Moderno)

**JAX** es otra opci√≥n interesante:
- Compilaci√≥n JIT autom√°tica
- Puede usar GPU/TPU autom√°ticamente
- API similar a NumPy
- Gradientes autom√°ticos (√∫til para optimizaci√≥n)

```python
import jax.numpy as jnp
from jax import jit, vmap

@jit
def pixelCalc_JAX(patch):
    m = jnp.mean(patch, axis=0)
    Cinv = jnp.linalg.inv(C)
    return m, Cinv

# Paralelizar autom√°ticamente
PACOCalc_vectorized = vmap(process_pixel)
```

---

## ‚úÖ Conclusi√≥n

**Dificultad General**: ‚≠ê‚≠ê‚≠ê (Media)

**Recomendaci√≥n**:
1. **Empezar con CuPy** (m√°s f√°cil, cambios m√≠nimos)
2. **Medir speedup real** con tus datos
3. **Optimizar si es necesario** con Numba CUDA

**Tiempo estimado para implementaci√≥n b√°sica**: 2-4 horas
**Speedup esperado**: 10-30x (dependiendo de GPU y datos)

**¬øVale la pena?**
- ‚úÖ **S√ç** si tienes GPU NVIDIA moderna (RTX 2060+)
- ‚úÖ **S√ç** si procesas muchas im√°genes
- ‚ö†Ô∏è **Quiz√°s** si solo procesas ocasionalmente
- ‚ùå **NO** si no tienes GPU o es muy antigua


