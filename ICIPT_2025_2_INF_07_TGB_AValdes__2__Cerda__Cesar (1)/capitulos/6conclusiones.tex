\section{Conclusiones del Primer Semestre}
\label{sec:conclusion_semestre_1}

El primer semestre de este proyecto ha sido dedicado a establecer las bases técnicas y teóricas necesarias para el desarrollo de una nueva metodología de detección y caracterización de exoplanetas. Se han logrado avances en el diseño del pipeline y se comenzará con el inicio de la implementación de los módulos centrales del sistema híbrido propuesto, lo que validará la viabilidad del enfoque, sentando las bases para la siguiente fase de integración.

En primer lugar, se comenzará la implementación de una versión del algoritmo PACO optimizada para la arquitectura de unidades de procesamiento gráfico utilizando CUDA. Este trabajo abordará la alta complejidad computacional inherente al cálculo de la matriz de covarianza local en la versión secuencial de PACO. Los resultados preliminares, que se obtendrán a través de pruebas con datos sintéticos, se espera sugieran que la paralelización del algoritmo a nivel de GPU reducirá el tiempo del proceso, lo cual será un factor crítico para el procesamiento eficiente de imágenes astronómicas de alta resolución. La implementación de kernels CUDA personalizados para tareas intensivas en paralelismo como el cálculo de covarianza, promete ser un enfoque eficaz para superar el cuello de botella computacional del algoritmo.

En segundo lugar, se espera desarrollar una adaptación del algoritmo RSM que incorpore técnicas de inferencia variacional. El objetivo de esta modificación será mitigar la elevada complejidad computacional de la versión estándar de RSM, que asciende a $\mathcal{O}(n^3)$. La implementación con VI se espera logre reducir esta complejidad a $\mathcal{O}(n)$, lo que transformaría a RSM en un algoritmo viable para el análisis de grandes volúmenes de datos. La validación inicial se hará comparando la aproximación variacional con un método de referencia basado en Cadenas de Markov Monte Carlo, lo que demostraría que la ganancia en eficiencia computacional no compromete de manera significativa la precisión del modelo y lo hará escalable.

Finalmente, la extensa revisión del estado del arte y la literatura asociada justifica la integración de PACO y RSM como una solución complementaria junto a las mejoras propuestas de cada algoritmo. La combinación de la capacidad de detección de objetos débiles de PACO con la robustez estadística de RSM en la supresión de speckless evidencian una propuesta prometedora para superar las limitaciones de los métodos actuales. El desarrollo de PACO con paralelización GPU utilizando CUDA, justificado con las bases teóricas de manejo y cálculo de matrices de covarianza y la mejora de RSM haciéndolo escalable manteniendo una alta precisión, nos permite concluir que en este semestre los objetivos iniciales fueron cumplidos y que los componentes individuales se encuentran justificados teóricamente a nivel de la literatura para ser desarrollados, mejorados e implementados dando paso a la integración del pipeline completo.

\section{Planificación del Segundo Semestre}
\label{sec:planificacion}

La siguiente fase del proyecto se centrará en la implementación, desarrollo de los algoritmos, integración, validación rigurosa y documentación final del sistema híbrido. Se ha diseñado un plan de trabajo estructurado en cuatro etapas principales:

La primera etapa consistirá en la mejora de los algoritmos base, como se ha propuesto, PACO en GPU y RSM con VI, utilizando en el caso de PACO tanto el repositorio base como la implementación en el paquete VIP, y en el caso de RSM modificando el algoritmo original a nivel teórico e implementándolo en python. Se espera realizar un benchmark de las implementaciones que justifique el desarrollo, y luego, se logre una implementación que pueda ser integrada directamente en el pipeline.

La segunda etapa consistirá en la integración del pipeline híbrido. Se unificarán los módulos PACO-GPU y RSM-VI, prestando especial atención al flujo de datos y a la optimización de la gestión de memoria en GPU. Se desarrollará una arquitectura de software que maximice la eficiencia computacional. La validación de esta etapa se realizará a través de pruebas de integración con datos sintéticos, asegurando que los módulos operen de manera conjunta y sin fallas.

La tercera  etapa se enfocará en la validación y evaluación del desempeño. El pipeline híbrido será sometido a un proceso de evaluación utilizando tanto datos sintéticos como datos reales provenientes de observatorios y telescopios como el VLT y el JWST. Se emplearán métricas de desempeño estándar en el campo, como la tasa de falsos positivos, la tasa de falsos negativos, y el análisis de la curva ROC, para cuantificar la capacidad de detección y caracterización del sistema. Adicionalmente, se realizarán nuevos benchmarks comparativos para contrastar el rendimiento del método híbrido con otras técnicas del estado del arte.

La etapa final se centrará en la documentación y el análisis de los resultados. Se procederá a la redacción del informe final, detallando el diseño, la implementación, los resultados experimentales y las conclusiones del proyecto. El código fuente desarrollado será documentado para asegurar su reproducibilidad. Finalmente, se preparará la presentación y defensa de título, sintetizando las principales contribuciones de la investigación.


