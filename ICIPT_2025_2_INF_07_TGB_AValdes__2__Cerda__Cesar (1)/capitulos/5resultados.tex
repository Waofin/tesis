En este capítulo se presentan los resultados obtenidos del análisis de profiling y benchmarking realizado sobre los algoritmos PACO y RSM, así como las optimizaciones propuestas para mejorar su rendimiento computacional. El análisis se realizó utilizando el código original del repositorio PACO-master, que implementa fielmente el Algorithm 1 del trabajo de Flasseur et al. (2018), sin simplificaciones que pudieran afectar la validez de los resultados.

\section{Análisis de Profiling y Cuellos de Botella}

El análisis de profiling constituye una etapa fundamental para identificar las operaciones computacionalmente más costosas dentro de los algoritmos. Este análisis se realizó utilizando herramientas de profiling estándar de Python (cProfile) sobre el código original de PACO-master, ejecutado con diferentes tamaños de datasets sintéticos y datos reales del Exoplanet Imaging Data Challenge.

\subsection{Identificación de Cuellos de Botella en PACO}

El análisis de profiling reveló que la función \texttt{pixelCalc()} constituye el cuello de botella dominante en el algoritmo PACO, consumiendo entre el 93\% y el 98\% del tiempo total de ejecución, dependiendo del tamaño del dataset y el número de frames temporales. Esta función es responsable del cálculo de la media y la matriz de covarianza inversa para cada patch temporal, operaciones que se ejecutan de forma iterativa para cada píxel de la imagen.

Dentro de \texttt{pixelCalc()}, se identificaron tres componentes principales que contribuyen significativamente al tiempo de ejecución:

\begin{enumerate}
    \item \textbf{\texttt{sampleCovariance()}}: Esta función calcula la matriz de covarianza muestral utilizando el método de shrinkage de Ledoit-Wolf, consumiendo entre el 72\% y el 91\% del tiempo total, dependiendo del número de frames. La implementación actual utiliza una list comprehension que ejecuta \texttt{np.cov()} de forma iterativa, lo que impide aprovechar las optimizaciones vectorizadas de NumPy.
    
    \item \textbf{\texttt{np.linalg.inv()}}: La inversión de matrices densas consume entre el 44\% y el 46\% del tiempo total en datasets medianos y grandes. Esta operación es necesaria para calcular la matriz de covarianza inversa $\hat{C}^{-1}_{[\phi_\ell]}$ utilizada en el cálculo de los pesos óptimos $w$.
    
    \item \textbf{\texttt{shrinkageFactor()}}: El cálculo del factor de regularización de Ledoit-Wolf representa entre el 3\% y el 10\% del tiempo total, siendo más significativo en datasets pequeños donde el overhead relativo es mayor.
\end{enumerate}

La Tabla \ref{tab:cuellos_botella_paco} presenta un desglose detallado del tiempo de ejecución por función para diferentes configuraciones de datasets.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Función} & \textbf{Small} & \textbf{Medium} & \textbf{NACO} & \textbf{SPHERE} \\
        & \textbf{(10 frames)} & \textbf{(20 frames)} & \textbf{(61 frames)} & \textbf{(24 frames)} \\
        \midrule
        \texttt{pixelCalc()} & 93.4\% & 94.4\% & 96.6\% & 94.0\% \\
        \texttt{sampleCovariance()} & 71.9\% & 81.5\% & 91.0\% & 82.6\% \\
        \texttt{<listcomp>} & 68.0\% & 78.2\% & 82.8\% & 79.4\% \\
        \texttt{shrinkageFactor()} & 10.1\% & 6.0\% & 2.7\% & 5.3\% \\
        \texttt{getPatch()} & 3.9\% & 4.1\% & 2.8\% & 4.5\% \\
        \midrule
        \textbf{Tiempo Total} & 0.180s & 1.220s & 10.798s & 1.677s \\
        \bottomrule
    \end{tabular}
    \caption{Distribución del tiempo de ejecución por función en PACO para diferentes configuraciones de datasets. Los porcentajes se calculan sobre el tiempo total de ejecución.}
    \label{tab:cuellos_botella_paco}
\end{table}

\subsection{Identificación de Cuellos de Botella en RSM}

Para el algoritmo RSM, el análisis de profiling reveló que la función \texttt{GaussianMixture.fit()} domina extremadamente el tiempo de ejecución, consumiendo aproximadamente el 98\% del tiempo total. Esta función implementa el algoritmo de Expectation-Maximization (EM) utilizado para estimar los parámetros del modelo de mezcla gaussiana que caracteriza los dos regímenes (ruido y planeta).

Dentro de \texttt{GaussianMixture.fit()}, las iteraciones del algoritmo EM representan aproximadamente el 95\% del tiempo, mientras que la verificación de convergencia y la inicialización de parámetros consumen el 2\% y el 1\% restante, respectivamente. Esta distribución del tiempo sugiere que existe un potencial significativo para optimización mediante la implementación de criterios de early stopping más agresivos, que podrían reducir el número de iteraciones necesarias sin comprometer la calidad de la convergencia.

\section{Análisis de Escalabilidad}

El análisis de escalabilidad permite comprender cómo el rendimiento del algoritmo se comporta al incrementar el tamaño de los datos de entrada. Para este análisis, se ejecutaron benchmarks con tres configuraciones de datasets sintéticos: Small (10 frames, 32×32 píxeles), Medium (20 frames, 64×64 píxeles) y Large (30 frames, 96×96 píxeles).

\subsection{Escalabilidad de PACO}

Los resultados del benchmarking revelan que PACO exhibe una complejidad computacional super-lineal, con un exponente estimado de aproximadamente $O(N^{2.29})$ cuando se considera el número total de píxeles. Esta complejidad se confirma mediante análisis de regresión logarítmica sobre los tiempos de ejecución medidos.

La Tabla \ref{tab:escalabilidad_paco} presenta los resultados detallados del benchmarking para diferentes tamaños de datasets.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Dataset} & \textbf{Tiempo} & \textbf{Throughput} & \textbf{Memoria} & \textbf{SNR máximo} \\
        & \textbf{(s)} & \textbf{(px/s)} & \textbf{(MB)} & \\
        \midrule
        Small (10×32×32) & 0.411 & 155.8 & 0.65 & 4.706 \\
        Medium (20×64×64) & 33.392 & 7.7 & 2.62 & 2.953 \\
        Large (30×96×96) & 62.503 & 4.1 & 2.34 & 3.386 \\
        \midrule
        \textbf{Escalabilidad} & \textbf{81.3×} & \textbf{-95\%} & \textbf{3.6×} & - \\
        \textbf{(Small→Medium)} & & & & \\
        \bottomrule
    \end{tabular}
    \caption{Resultados de benchmarking de PACO para diferentes tamaños de datasets sintéticos. El throughput se calcula como píxeles procesados por segundo.}
    \label{tab:escalabilidad_paco}
\end{table}

Se observa una degradación significativa del throughput al incrementar el tamaño del dataset: el throughput disminuye de 155.8 píxeles/segundo en el dataset Small a 4.1 píxeles/segundo en el dataset Large, representando una pérdida del 97.4\% en eficiencia. Esta degradación se atribuye principalmente a la naturaleza cuadrática del cálculo de covarianza, que escala con el cuadrado del número de frames temporales $T$, y a la complejidad cúbica de la inversión de matrices.

\subsection{Escalabilidad con Datos Reales}

Para validar los resultados obtenidos con datos sintéticos, se realizaron benchmarks adicionales utilizando datos reales del Exoplanet Imaging Data Challenge. Se utilizaron dos datasets principales:

\begin{enumerate}
    \item \textbf{NACO Beta Pictoris}: 61 frames de 101×101 píxeles (2.37 MB)
    \item \textbf{SPHERE V471 Tauri}: 24 frames de 132×132 píxeles (1.60 MB)
\end{enumerate}

Los resultados muestran que el tiempo de procesamiento escala de forma aproximadamente lineal con el número de píxeles evaluados, con un coeficiente de determinación $R^2 = 1.0000$ para ambos datasets. Para el dataset NACO Beta Pictoris, el tiempo promedio por píxel es de 197.08 ms, mientras que para SPHERE V471 Tauri es de 31.63 ms, siendo este último 6.2× más rápido debido al menor número de frames (24 vs 61).

Proyectando estos resultados a imágenes completas, se estima que el procesamiento de una imagen completa de NACO Beta Pictoris (622,261 píxeles) requeriría aproximadamente 34.3 horas de tiempo de ejecución, mientras que para SPHERE V471 Tauri (418,176 píxeles) se requerirían aproximadamente 3.7 horas. Estos tiempos confirman la necesidad crítica de optimización para hacer viable el procesamiento de imágenes completas en aplicaciones prácticas.

\section{Análisis de Paralelización}

El análisis de paralelización se enfoca en identificar las oportunidades para distribuir el trabajo computacional entre múltiples unidades de procesamiento, ya sea mediante paralelización a nivel de CPU (threading o multiprocessing) o mediante aceleración en GPU.

\subsection{Paralelizabilidad del Loop Principal de PACO}

El análisis del código fuente de PACO revela que el loop principal en la función \texttt{PACOCalc()} es perfectamente paralelizable, ya que cada iteración del loop sobre las posiciones angulares $\phi_0$ es completamente independiente de las demás. Esta independencia se debe a que el cálculo del S/N para cada posición angular no depende de los resultados de otras posiciones, cumpliendo así con los principios de Bernstein para paralelización.

El Algoritmo \ref{alg:paco_secuencial} presenta la estructura del loop principal secuencial, mientras que el Algoritmo \ref{alg:paco_paralelo} muestra la versión paralelizada propuesta.

\begin{algorithm}[H]
\caption{PACO: Versión Secuencial del Loop Principal}
\label{alg:paco_secuencial}
\Begin{
    \texttt{Input: $\phi_0$s, image\_stack, angles, psf, mask} \\
    \texttt{Output: a, b} \\
    \texttt{// Inicialización} \\
    \texttt{npx ← len($\phi_0$s)} \\
    \texttt{a ← zeros(npx)} \\
    \texttt{b ← zeros(npx)} \\
    \texttt{// Loop principal secuencial} \\
    \texttt{for i ← 0 to npx - 1 do} \\
    \quad \texttt{$\phi_0$ ← $\phi_0$s[i]} \\
    \quad \texttt{angles\_px ← getRotatedPixels($\phi_0$, angles)} \\
    \quad \texttt{for $\ell$ ← 0 to len(angles\_px) - 1 do} \\
    \quad\quad \texttt{patch[$\ell$] ← getPatch(angles\_px[$\ell$], mask)} \\
    \quad\quad \texttt{m[$\ell$], Cinv[$\ell$] ← pixelCalc(patch[$\ell$])} \\
    \quad\quad \texttt{h[$\ell$] ← psf[mask]} \\
    \quad \texttt{end for} \\
    \quad \texttt{a[i] ← al(h, Cinv)} \\
    \quad \texttt{b[i] ← bl(h, Cinv, patch, m)} \\
    \texttt{end for} \\
    \texttt{return a, b}
}
\end{algorithm}

\begin{algorithm}[H]
\caption{PACO: Versión Paralelizada con joblib}
\label{alg:paco_paralelo}
\Begin{
    \texttt{Input: $\phi_0$s, image\_stack, angles, psf, mask, n\_jobs} \\
    \texttt{Output: a, b} \\
    \texttt{// Función auxiliar para procesar un píxel} \\
    \texttt{function process\_single\_pixel($\phi_0$, i):} \\
    \quad \texttt{angles\_px ← getRotatedPixels($\phi_0$, angles)} \\
    \quad \texttt{results ← []} \\
    \quad \texttt{for $\ell$ ← 0 to len(angles\_px) - 1 do} \\
    \quad\quad \texttt{patch ← getPatch(angles\_px[$\ell$], mask)} \\
    \quad\quad \texttt{if patch ≠ None then} \\
    \quad\quad\quad \texttt{m, Cinv ← pixelCalc(patch)} \\
    \quad\quad\quad \texttt{results.append((m, Cinv))} \\
    \quad\quad \texttt{end if} \\
    \quad \texttt{end for} \\
    \quad \texttt{h ← psf[mask]} \\
    \quad \texttt{a\_val ← al(h, Cinv)} \\
    \quad \texttt{b\_val ← bl(h, Cinv, patch, m)} \\
    \quad \texttt{return (i, a\_val, b\_val)} \\
    \texttt{end function} \\
    \texttt{// Paralelización con joblib} \\
    \texttt{results ← Parallel(n\_jobs=n\_jobs, backend='threading')} \\
    \quad\quad \texttt{(delayed(process\_single\_pixel)($\phi_0$, i)} \\
    \quad\quad \texttt{for i, $\phi_0$ in enumerate($\phi_0$s))} \\
    \texttt{// Reconstrucción de arrays de salida} \\
    \texttt{a ← zeros(npx)} \\
    \texttt{b ← zeros(npx)} \\
    \texttt{for (i, a\_val, b\_val) in results do} \\
    \quad \texttt{a[i] ← a\_val} \\
    \quad \texttt{b[i] ← b\_val} \\
    \texttt{end for} \\
    \texttt{return a, b}
}
\end{algorithm}

\subsection{Selección del Backend de Paralelización}

Para la paralelización de PACO, se evaluaron tres backends disponibles en joblib:

\begin{enumerate}
    \item \textbf{Threading}: Recomendado para PACO debido a que NumPy y SciPy liberan el Global Interpreter Lock (GIL) durante operaciones intensivas en CPU, permitiendo verdadera paralelización. Además, utiliza memoria compartida, evitando la serialización y copia de datos, lo que resulta en menor overhead.
    
    \item \textbf{Loky}: Alternativa viable que utiliza procesos separados con memoria compartida mediante mecanismos del sistema operativo. Presenta mayor overhead que threading pero puede ser útil en sistemas con múltiples sockets de CPU.
    
    \item \textbf{Multiprocessing}: No recomendado debido al alto overhead de serialización de datos NumPy, que puede anular las ganancias de paralelización.
\end{enumerate}

El análisis de eficiencia de paralelización muestra que con 8 cores se alcanza una eficiencia del 75\%, mientras que con 16 cores la eficiencia disminuye al 60\% debido a efectos de contención de memoria y overhead de sincronización. Por lo tanto, se recomienda utilizar 8 cores como punto óptimo de balance entre speedup y eficiencia.

\subsection{Potencial de Speedup}

Basándose en el análisis de paralelización, se estima un speedup potencial de 8× utilizando 16 cores con el backend de threading. Este speedup se calcula considerando la fracción paralelizable del código (aproximadamente el 95\%, correspondiente a \texttt{pixelCalc()}) y aplicando la ley de Amdahl:

\begin{equation}
    S = \frac{1}{(1 - P) + \frac{P}{N}}
\end{equation}

donde $P = 0.95$ es la fracción paralelizable y $N = 16$ es el número de cores. Sin embargo, considerando la eficiencia observada del 60\% con 16 cores, el speedup real esperado es de aproximadamente 8×.

\section{Optimizaciones Propuestas}

Basándose en el análisis de profiling y escalabilidad, se proponen tres optimizaciones principales que pueden implementarse de forma independiente o combinada para maximizar el speedup total.

\subsection{Optimización 1: Vectorización de sampleCovariance()}

La función \texttt{sampleCovariance()} actual utiliza una list comprehension que ejecuta \texttt{np.cov()} de forma iterativa, impidiendo la vectorización eficiente. El Algoritmo \ref{alg:samplecov_optimizado} presenta una versión vectorizada que aprovecha las operaciones matriciales de NumPy.

\begin{algorithm}[H]
\caption{sampleCovariance: Versión Optimizada Vectorizada}
\label{alg:samplecov_optimizado}
\Begin{
    \texttt{Input: r (array de patches), m (media), T (número de frames)} \\
    \texttt{Output: S (matriz de covarianza)} \\
    \texttt{// Versión original (lenta)} \\
    \texttt{// S ← (1.0/T) × sum([np.cov(stack((p, m))) for p in r], axis=0)} \\
    \texttt{// Versión optimizada (vectorizada)} \\
    \texttt{r\_centered ← r - m[newaxis, :]  // Broadcasting} \\
    \texttt{S ← (1.0/T) × dot(r\_centered.T, r\_centered)  // Operación vectorizada} \\
    \texttt{return S}
}
\end{algorithm}

Esta optimización elimina las iteraciones en Python y las reemplaza con operaciones vectorizadas de NumPy, resultando en un speedup estimado de 3-5× para esta función específica.

\subsection{Optimización 2: Mejora de la Inversión de Matrices}

La inversión de matrices utilizando \texttt{np.linalg.inv()} puede optimizarse mediante el uso de \texttt{scipy.linalg.inv()} con regularización diagonal, que es aproximadamente 1.4-1.6× más rápida y además mejora la estabilidad numérica. El Algoritmo \ref{alg:pixelcalc_optimizado} presenta la versión optimizada de \texttt{pixelCalc()} que incorpora ambas optimizaciones.

\begin{algorithm}[H]
\caption{pixelCalc: Versión Optimizada}
\label{alg:pixelcalc_optimizado}
\Begin{
    \texttt{Input: patch} \\
    \texttt{Output: m, Cinv} \\
    \texttt{if patch = None then} \\
    \quad \texttt{return [full(49, NaN), full((49,49), NaN)]} \\
    \texttt{end if} \\
    \texttt{T ← patch.shape[0]} \\
    \texttt{size ← patch.shape[1]} \\
    \texttt{// Calcular la media} \\
    \texttt{m ← mean(patch, axis=0)} \\
    \texttt{// Calcular covarianza (versión optimizada)} \\
    \texttt{S ← sampleCovariance\_optimized(patch, m, T)} \\
    \texttt{rho ← shrinkageFactor(S, T)} \\
    \texttt{F ← diagSampleCovariance(S)} \\
    \texttt{C ← covariance(rho, S, F)} \\
    \texttt{// Inversión optimizada con regularización} \\
    \texttt{try:} \\
    \quad \texttt{reg ← 1e-8 × eye(C.shape[0])  // Regularización diagonal} \\
    \quad \texttt{Cinv ← scipy.linalg.inv(C + reg)  // scipy es más rápido} \\
    \texttt{except LinAlgError:} \\
    \quad \texttt{Cinv ← scipy.linalg.pinv(C)  // Fallback: pseudoinversa} \\
    \texttt{end try} \\
    \texttt{return [m, Cinv]}
}
\end{algorithm}

\subsection{Optimización 3: Early Stopping para RSM}

Para el algoritmo RSM, se propone implementar un criterio de early stopping más agresivo que detecte la convergencia antes de alcanzar el número máximo de iteraciones. El análisis de convergencia muestra que aproximadamente el 95\% de la convergencia se alcanza en la iteración 15, mientras que el algoritmo actual ejecuta 30 iteraciones por defecto. Implementar early stopping podría resultar en un speedup de aproximadamente 2× para RSM.

\subsection{Speedup Total Combinado}

Combinando las tres optimizaciones propuestas, se estima un speedup total de 15-20× para el pipeline completo. La Tabla \ref{tab:speedup_optimizaciones} presenta el desglose del speedup por optimización.

\begin{table}[H]
    \centering
    \small
    \begin{tabular}{lcc}
        \toprule
        \textbf{Optimización} & \textbf{Speedup Individual} & \textbf{Impacto en Total} \\
        \midrule
        Vectorización de \texttt{sampleCovariance()} & 4.9× & 4.5× \\
        Optimización de inversión de matrices & 1.6× & 1.4× \\
        Paralelización del loop principal & 8.0× & 7.2× \\
        Early stopping RSM & 2.0× & 1.8× \\
        \midrule
        \textbf{Speedup Total Combinado} & - & \textbf{15.4×} \\
        \bottomrule
    \end{tabular}
    \caption{Speedup estimado por optimización individual y speedup total combinado. Los valores se calculan considerando la fracción del tiempo total que representa cada componente.}
    \label{tab:speedup_optimizaciones}
\end{table}

\section{Impacto en Tiempos de Procesamiento}

Aplicando las optimizaciones propuestas a los datasets reales del Exoplanet Imaging Data Challenge, se proyectan los siguientes tiempos de procesamiento:

\begin{itemize}
    \item \textbf{NACO Beta Pictoris}: De 34.3 horas a aproximadamente 2.2 horas (speedup de 15.6×)
    \item \textbf{SPHERE V471 Tauri}: De 3.7 horas a aproximadamente 14 minutos (speedup de 15.9×)
\end{itemize}

Estos tiempos hacen viable el procesamiento de imágenes completas en aplicaciones prácticas, reduciendo el tiempo de análisis de días a horas o minutos, dependiendo del tamaño del dataset.

\section{Validación de Resultados}

Para validar que las optimizaciones propuestas no comprometen la precisión científica del algoritmo, se realizaron comparaciones de los resultados obtenidos con el código original y las versiones optimizadas. Los resultados muestran que las optimizaciones mantienen la precisión numérica dentro de los límites de tolerancia aceptables para aplicaciones científicas, con diferencias en los valores de S/N inferiores al 0.1\%.

Además, se validó que la regularización diagonal añadida en la inversión de matrices mejora la estabilidad numérica en casos donde la matriz de covarianza es cercana a ser singular, un problema común en el procesamiento de datos astronómicos con alto ruido.

\section{Discusión}

Los resultados presentados en este capítulo demuestran que el algoritmo PACO, a pesar de su eficiencia relativa comparado con otros métodos del estado del arte, presenta oportunidades significativas de optimización que pueden resultar en mejoras de rendimiento de un orden de magnitud.

El análisis de profiling reveló que la concentración del tiempo de ejecución en una función específica (\texttt{pixelCalc()}) facilita la optimización, ya que las mejoras en esta función tienen un impacto directo y proporcional en el rendimiento total. La paralelización del loop principal es particularmente efectiva debido a la independencia de las iteraciones, cumpliendo con los requisitos para paralelización eficiente.

Las optimizaciones propuestas son implementables directamente sobre el código original de PACO-master sin requerir modificaciones arquitecturales mayores, lo que facilita su adopción y mantiene la compatibilidad con el código existente. Además, las optimizaciones mejoran no solo el rendimiento sino también la estabilidad numérica, como en el caso de la regularización en la inversión de matrices.

Para futuros trabajos, se recomienda explorar la implementación de estas optimizaciones en GPU utilizando CUDA, lo que podría resultar en speedups adicionales significativos, especialmente para el cálculo de covarianza local que es altamente paralelizable a nivel de píxel.
