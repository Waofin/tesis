{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark y Análisis Completo de PACO en VIP-master\n",
    "\n",
    "Este notebook realiza:\n",
    "1. **Análisis de Errores**: Detecta problemas en la implementación de PACO\n",
    "2. **Carga de Datasets**: Descarga datasets desde GitHub (subchallenge1)\n",
    "3. **Benchmark Completo**: Mide tiempos de ejecución con diferentes datasets\n",
    "4. **Profiling**: Analiza el rendimiento del algoritmo\n",
    "5. **Visualizaciones**: Genera gráficos y reportes\n",
    "\n",
    "## Configuración para Colab\n",
    "\n",
    "1. **Seleccionar Runtime**: Runtime > Change runtime type > Hardware accelerator: GPU (opcional)\n",
    "2. **Ejecutar todas las celdas**: Runtime > Run all\n",
    "\n",
    "## Autor\n",
    "César Cerda - Universidad del Bío-Bío\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instalación de Dependencias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "!pip install -q astropy scipy matplotlib ipywidgets\n",
    "!pip install -q memory-profiler line-profiler\n",
    "\n",
    "print(\"[OK] Dependencias instaladas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clonar Repositorios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clonar repositorios necesarios\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# URLs de los repositorios\n",
    "VIP_REPO_URL = \"https://github.com/vortex-exoplanet/VIP.git\"\n",
    "TESIS_REPO_URL = \"https://github.com/Waofin/tesis.git\"\n",
    "\n",
    "# Directorios de destino\n",
    "REPOS_DIR = Path(\"/content/repos\")\n",
    "VIP_DIR = REPOS_DIR / \"VIP\"\n",
    "TESIS_DIR = REPOS_DIR / \"tesis\"\n",
    "\n",
    "# Crear directorio de repositorios\n",
    "REPOS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CLONANDO REPOSITORIOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Clonar VIP si no existe\n",
    "if not VIP_DIR.exists():\n",
    "    print(f\"Clonando VIP desde {VIP_REPO_URL}...\")\n",
    "    !cd {REPOS_DIR} && git clone {VIP_REPO_URL}\n",
    "    print(\"[OK] VIP clonado correctamente\")\n",
    "else:\n",
    "    print(\"[OK] VIP ya existe\")\n",
    "\n",
    "# Clonar repositorio de tesis (para datasets)\n",
    "# IMPORTANTE: Los archivos están en Git LFS, necesitamos clonar con LFS habilitado\n",
    "if not TESIS_DIR.exists():\n",
    "    print(f\"\\nClonando repositorio de tesis desde {TESIS_REPO_URL}...\")\n",
    "    print(\"  (Los archivos están en Git LFS, instalando git-lfs si es necesario...)\")\n",
    "    \n",
    "    # Instalar git-lfs si no está disponible\n",
    "    print(\"  Instalando Git LFS...\")\n",
    "    !apt-get update -qq 2>&1 | head -3\n",
    "    !apt-get install -y git-lfs 2>&1 | tail -3\n",
    "    !git lfs install 2>&1\n",
    "    \n",
    "    # Clonar con LFS\n",
    "    print(\"  Clonando repositorio...\")\n",
    "    !cd {REPOS_DIR} && git clone {TESIS_REPO_URL} 2>&1 | tail -5\n",
    "    \n",
    "    # Intentar descargar archivos LFS\n",
    "    if TESIS_DIR.exists():\n",
    "        print(\"  Descargando archivos LFS (esto puede tardar varios minutos)...\")\n",
    "        print(\"  Por favor espera, los archivos son grandes...\")\n",
    "        !cd {TESIS_DIR} && git lfs pull 2>&1 | tail -10\n",
    "        print(\"  [OK] Descarga de LFS completada\")\n",
    "    \n",
    "    print(\"[OK] Repositorio de tesis clonado correctamente\")\n",
    "else:\n",
    "    print(\"[OK] Repositorio de tesis ya existe\")\n",
    "    # Verificar si los archivos LFS están descargados\n",
    "    print(\"  Verificando archivos LFS...\")\n",
    "    test_file = TESIS_DIR / \"subchallenge1\" / \"sphere_irdis_1_cube.fits\"\n",
    "    if test_file.exists():\n",
    "        # Verificar si es puntero\n",
    "        try:\n",
    "            with open(test_file, 'rb') as f:\n",
    "                first_bytes = f.read(100)\n",
    "                if b'git-lfs' in first_bytes or b'version https://git-lfs' in first_bytes:\n",
    "                                        print(\"  [WARNING] Los archivos son punteros LFS, descargando contenido real...\")\n",
    "                    !cd {TESIS_DIR} && git lfs pull 2>&1 | tail -10\n",
    "                else:\n",
    "                    print(\"  [OK] Los archivos LFS ya estan descargados\")\n",
    "            except:\n",
    "                print(\"  [OK] Archivos encontrados\")\n",
    "    else:\n",
    "        print(\"  [WARNING] Archivos no encontrados, intentando descargar LFS...\")\n",
    "        !cd {TESIS_DIR} && git lfs pull 2>&1 | tail -10\n",
    "\n",
    "print(f\"\\n[OK] Repositorios disponibles en: {REPOS_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Instalar VIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar VIP desde el repositorio clonado\n",
    "print(\"=\"*70)\n",
    "print(\"INSTALANDO VIP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if VIP_DIR.exists():\n",
    "    print(f\"Instalando VIP desde {VIP_DIR}...\")\n",
    "    !cd {VIP_DIR} && pip install -q -e .\n",
    "    print(\"[OK] VIP instalado correctamente\")\n",
    "else:\n",
    "    print(\"[WARNING] VIP no encontrado, instalando desde PyPI...\")\n",
    "    !pip install -q vip-hci\n",
    "    print(\"[OK] VIP instalado desde PyPI\")\n",
    "\n",
    "# Verificar instalación\n",
    "import sys\n",
    "vip_src = VIP_DIR / \"src\"\n",
    "if vip_src.exists():\n",
    "    if str(vip_src) not in sys.path:\n",
    "        sys.path.insert(0, str(vip_src))\n",
    "    print(f\"[OK] VIP agregado al path: {vip_src}\")\n",
    "\n",
    "try:\n",
    "    import vip_hci as vip\n",
    "    vip_version = vip.__version__ if hasattr(vip, '__version__') else 'desconocida'\n",
    "    print(f\"[OK] VIP importado correctamente (version: {vip_version})\")\n",
    "except ImportError as e:\n",
    "    print(f\"[ERROR] Error importando VIP: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importar Librerías y Cargar Funciones de Benchmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import urllib.request\n",
    "from astropy.io import fits\n",
    "import pandas as pd\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Profiling\n",
    "try:\n",
    "    import cProfile\n",
    "    import pstats\n",
    "    from io import StringIO\n",
    "    PROFILING_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PROFILING_AVAILABLE = False\n",
    "\n",
    "# Detectar GPU\n",
    "try:\n",
    "    import torch\n",
    "    GPU_AVAILABLE = torch.cuda.is_available()\n",
    "    if GPU_AVAILABLE:\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        print(f\"[OK] GPU disponible: {gpu_name}\")\n",
    "    else:\n",
    "        print(\"[WARNING] GPU no disponible, usando CPU\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"[WARNING] PyTorch no disponible\")\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CONFIGURACIÓN COMPLETA\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar funciones del script de benchmark\n",
    "# Si el script está disponible, lo importamos; si no, definimos las funciones aquí\n",
    "try:\n",
    "    # Intentar cargar desde el script si está en el mismo directorio\n",
    "    sys.path.insert(0, str(Path.cwd()))\n",
    "    from benchmark_paco_vip import (\n",
    "        load_challenge_dataset,\n",
    "        benchmark_paco,\n",
    "        PACOErrorDetector\n",
    "    )\n",
    "    print(\"[OK] Funciones de benchmark cargadas desde script\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] Script no encontrado, las funciones se definiran en las siguientes celdas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Análisis de Errores y Cargar/Ejecutar Benchmark\n",
    "\n",
    "**[WARNING] IMPORTANTE**: \n",
    "- Si obtienes errores 404 o \"Descargando desde URL\", **reinicia el kernel** (Kernel > Restart) y ejecuta todas las celdas desde el inicio.\n",
    "- Las funciones deben usar el repositorio clonado localmente, NO descargar desde URLs.\n",
    "- Asegúrate de haber ejecutado la celda 2 (Clonar Repositorios) antes de esta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si las funciones no se importaron, las definimos aquí\n",
    "# IMPORTANTE: Los archivos están en Git LFS, por lo que debemos usar el repositorio clonado\n",
    "# NO descargamos desde URLs raw porque los archivos están en Git LFS\n",
    "\n",
    "# Limpiar cualquier definición anterior de la función\n",
    "if 'load_challenge_dataset_simple' in globals():\n",
    "    del load_challenge_dataset_simple\n",
    "if 'benchmark_paco_simple' in globals():\n",
    "    del benchmark_paco_simple\n",
    "\n",
    "REPO_URL = \"https://github.com/Waofin/tesis\"\n",
    "\n",
    "# Verificar que TESIS_DIR esté definido\n",
    "if 'TESIS_DIR' not in globals():\n",
    "    raise NameError(\"TESIS_DIR no está definido. Ejecuta la celda 2 (Clonar Repositorios) primero.\")\n",
    "\n",
    "# Función simplificada para cargar datasets\n",
    "# Usa el repositorio clonado en lugar de descargar desde GitHub (los archivos están en Git LFS)\n",
    "def load_challenge_dataset_simple(repo_url, instrument, dataset_id, local_dir=None, branch='main'):\n",
    "    \"\"\"\n",
    "    Carga un dataset completo desde el repositorio clonado.\n",
    "    \n",
    "    NOTA: Los archivos están en Git LFS, por lo que deben estar en el repositorio clonado.\n",
    "    Si el repositorio no está clonado con Git LFS, los archivos serán punteros.\n",
    "    \"\"\"\n",
    "    # Usar el repositorio clonado en lugar de descargar\n",
    "    # El repositorio debería estar en /content/repos/tesis\n",
    "    repo_local_path = TESIS_DIR / \"subchallenge1\"\n",
    "    \n",
    "    if not repo_local_path.exists():\n",
    "        raise ValueError(f\"Repositorio no encontrado en {repo_local_path}. \"\n",
    "                        f\"Asegúrate de haber clonado el repositorio en la celda 2.\")\n",
    "    \n",
    "    # El formato correcto es: {instrument}_cube_{dataset_id}.fits\n",
    "    # NO: {instrument}_{dataset_id}_cube.fits\n",
    "    files = {\n",
    "        'cube': f\"{instrument}_cube_{dataset_id}.fits\",\n",
    "        'pa': f\"{instrument}_pa_{dataset_id}.fits\",\n",
    "        'psf': f\"{instrument}_psf_{dataset_id}.fits\",\n",
    "        'pxscale': f\"{instrument}_pxscale_{dataset_id}.fits\"\n",
    "    }\n",
    "    \n",
    "    print(f\"Cargando {instrument}_{dataset_id} desde repositorio clonado...\")\n",
    "    print(f\"  Ruta: {repo_local_path}\")\n",
    "    \n",
    "    # Verificar que el repositorio existe\n",
    "    if not TESIS_DIR.exists():\n",
    "        raise ValueError(f\"ERROR: El repositorio no está clonado en {TESIS_DIR}. \"\n",
    "                        f\"Por favor ejecuta la celda 2 primero para clonar el repositorio.\")\n",
    "    \n",
    "    if not repo_local_path.exists():\n",
    "        raise ValueError(f\"ERROR: La carpeta subchallenge1 no existe en {repo_local_path}. \"\n",
    "                        f\"Verifica que el repositorio se clonó correctamente.\")\n",
    "    \n",
    "    # Listar archivos disponibles para debug\n",
    "    available_files = list(repo_local_path.glob(\"*.fits\"))\n",
    "    print(f\"  Archivos .fits encontrados en la carpeta: {len(available_files)}\")\n",
    "    if len(available_files) > 0:\n",
    "        print(f\"  Primeros archivos: {[f.name for f in available_files[:5]]}\")\n",
    "    \n",
    "    # Verificar y descargar archivos LFS si es necesario\n",
    "    print(\"  Verificando archivos LFS...\")\n",
    "    import subprocess\n",
    "    \n",
    "    # Intentar instalar git-lfs si no está disponible\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'lfs', 'version'], \n",
    "                              capture_output=True, text=True, timeout=5)\n",
    "        if result.returncode != 0:\n",
    "            print(\"  Instalando Git LFS...\")\n",
    "            !apt-get update -qq && apt-get install -y git-lfs 2>&1 | head -5\n",
    "            !git lfs install 2>&1\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Git LFS check fallo: {e}\")\n",
    "    \n",
    "    # Intentar descargar archivos LFS\n",
    "    print(\"  Descargando archivos LFS (esto puede tardar)...\")\n",
    "    try:\n",
    "        result = subprocess.run(['git', 'lfs', 'pull'], \n",
    "                              cwd=str(TESIS_DIR), \n",
    "                              capture_output=True, \n",
    "                              text=True,\n",
    "                              timeout=120)\n",
    "        if result.returncode == 0:\n",
    "            print(\"  [OK] Archivos LFS descargados correctamente\")\n",
    "        else:\n",
    "            print(f\"  [WARNING] Git LFS pull fallo: {result.stderr[:200]}\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"  [WARNING] Git LFS pull tardo demasiado, continuando...\")\n",
    "    except Exception as e:\n",
    "        print(f\"  [WARNING] Error ejecutando git lfs pull: {e}\")\n",
    "    \n",
    "    # Cargar archivos desde el repositorio clonado\n",
    "    cube_path = repo_local_path / files['cube']\n",
    "    \n",
    "    # Verificar si el archivo existe o es un puntero LFS\n",
    "    if not cube_path.exists():\n",
    "        # Buscar archivos similares\n",
    "        similar = list(repo_local_path.glob(f\"*{instrument}*{dataset_id}*.fits\"))\n",
    "        if similar:\n",
    "            print(f\"  [WARNING] Archivo exacto no encontrado, pero hay archivos similares: {[f.name for f in similar]}\")\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {cube_path}\\n\"\n",
    "                               f\"Archivos disponibles: {[f.name for f in available_files if instrument in f.name]}\")\n",
    "    \n",
    "    # Verificar si es un puntero LFS\n",
    "    is_lfs_pointer = False\n",
    "    try:\n",
    "        with open(cube_path, 'rb') as f:\n",
    "            first_bytes = f.read(100)\n",
    "            first_line = first_bytes.decode('utf-8', errors='ignore')\n",
    "            if 'git-lfs' in first_line or 'version https://git-lfs' in first_line:\n",
    "                is_lfs_pointer = True\n",
    "                print(f\"  [WARNING] Archivo {files['cube']} es un puntero LFS\")\n",
    "    except:\n",
    "        pass  # Si no es texto, probablemente es un archivo binario válido\n",
    "    \n",
    "    if is_lfs_pointer:\n",
    "        print(\"  Intentando descargar archivos LFS nuevamente...\")\n",
    "        try:\n",
    "            subprocess.run(['git', 'lfs', 'pull', '--include', f'subchallenge1/{files[\"cube\"]}'], \n",
    "                         cwd=str(TESIS_DIR), \n",
    "                         check=False,\n",
    "                         timeout=60)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(f\"  Cargando cube: {files['cube']}\")\n",
    "    cube = fits.getdata(cube_path)\n",
    "    print(f\"    [OK] Cube shape: {cube.shape}\")\n",
    "    \n",
    "    pa_path = repo_local_path / files['pa']\n",
    "    if not pa_path.exists():\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {pa_path}\")\n",
    "    print(f\"  Cargando angulos: {files['pa']}\")\n",
    "    angles = fits.getdata(pa_path).flatten()\n",
    "    print(f\"    [OK] Angulos shape: {angles.shape}\")\n",
    "    \n",
    "    psf_path = repo_local_path / files['psf']\n",
    "    if not psf_path.exists():\n",
    "        raise FileNotFoundError(f\"Archivo no encontrado: {psf_path}\")\n",
    "    print(f\"  Cargando PSF: {files['psf']}\")\n",
    "    psf = fits.getdata(psf_path)\n",
    "    print(f\"    [OK] PSF shape: {psf.shape}\")\n",
    "    \n",
    "    pxscale_path = repo_local_path / files['pxscale']\n",
    "    if pxscale_path.exists():\n",
    "        print(f\"  Cargando pixel scale: {files['pxscale']}\")\n",
    "        pixscale = float(fits.getdata(pxscale_path).flatten()[0])\n",
    "        print(f\"    [OK] Pixel scale: {pixscale} arcsec/pixel\")\n",
    "    else:\n",
    "        print(f\"  [WARNING] Pixel scale no encontrado, usando 0.027 por defecto\")\n",
    "        pixscale = 0.027\n",
    "    \n",
    "    print(f\"[OK] Dataset cargado completamente\")\n",
    "    return cube, angles, psf, pixscale\n",
    "\n",
    "# Función simplificada de benchmark\n",
    "def benchmark_paco_simple(cube, angles, psf, pixscale, fwhm_arcsec, verbose=True):\n",
    "    \"\"\"Ejecuta benchmark de PACO\"\"\"\n",
    "    results = {'success': False, 'time': None, 'n_pixels': 0, 'errors': []}\n",
    "    \n",
    "    try:\n",
    "        sys.path.insert(0, str(VIP_DIR / \"src\"))\n",
    "        from vip_hci.invprob.paco import FastPACO\n",
    "        \n",
    "        # Recortar si es muy grande\n",
    "        if cube.shape[1] > 200 or cube.shape[2] > 200:\n",
    "            center_y, center_x = cube.shape[1] // 2, cube.shape[2] // 2\n",
    "            half_size = 50\n",
    "            cube = cube[:, center_y-half_size:center_y+half_size, center_x-half_size:center_x+half_size]\n",
    "        \n",
    "        vip_paco = FastPACO(cube=cube, angles=angles, psf=psf, fwhm=fwhm_arcsec, pixscale=pixscale, verbose=verbose)\n",
    "        \n",
    "        img_size = min(cube.shape[1], cube.shape[2])\n",
    "        center = img_size // 2\n",
    "        test_radius = min(5, center - 5)\n",
    "        y_coords, x_coords = np.meshgrid(\n",
    "            np.arange(center - test_radius, center + test_radius),\n",
    "            np.arange(center - test_radius, center + test_radius)\n",
    "        )\n",
    "        phi0s = np.column_stack((x_coords.flatten(), y_coords.flatten()))\n",
    "        \n",
    "        results['n_pixels'] = len(phi0s)\n",
    "        start_time = time.time()\n",
    "        a, b = vip_paco.PACOCalc(phi0s, use_subpixel_psf_astrometry=False, cpu=1)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        results['success'] = True\n",
    "        results['time'] = elapsed\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            snr = np.divide(b, np.sqrt(a), out=np.zeros_like(b), where=(a > 0))\n",
    "            snr = np.nan_to_num(snr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        results['snr_max'] = float(np.nanmax(snr))\n",
    "        results['snr_mean'] = float(np.nanmean(snr))\n",
    "        results['snr'] = snr\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[OK] Tiempo: {elapsed:.2f}s, SNR max: {results['snr_max']:.2f}\")\n",
    "    except Exception as e:\n",
    "        results['errors'].append(str(e))\n",
    "        print(f\"[ERROR] Error: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Función de profiling detallado\n",
    "def profile_paco_detailed(cube, angles, psf, pixscale, fwhm_arcsec, phi0s=None):\n",
    "    \"\"\"\n",
    "    Ejecuta profiling detallado de PACO con desglose por función\n",
    "    \"\"\"\n",
    "    import cProfile\n",
    "    import pstats\n",
    "    from io import StringIO\n",
    "    import pandas as pd\n",
    "    \n",
    "    sys.path.insert(0, str(VIP_DIR / \"src\"))\n",
    "    from vip_hci.invprob.paco import FastPACO\n",
    "    \n",
    "    # Recortar si es muy grande\n",
    "    if cube.shape[1] > 200 or cube.shape[2] > 200:\n",
    "        center_y, center_x = cube.shape[1] // 2, cube.shape[2] // 2\n",
    "        half_size = 50\n",
    "        cube = cube[:, center_y-half_size:center_y+half_size, center_x-half_size:center_x+half_size]\n",
    "    \n",
    "    vip_paco = FastPACO(cube=cube, angles=angles, psf=psf, fwhm=fwhm_arcsec, pixscale=pixscale, verbose=False)\n",
    "    \n",
    "    if phi0s is None:\n",
    "        img_size = min(cube.shape[1], cube.shape[2])\n",
    "        center = img_size // 2\n",
    "        test_radius = min(5, center - 5)\n",
    "        y_coords, x_coords = np.meshgrid(\n",
    "            np.arange(center - test_radius, center + test_radius),\n",
    "            np.arange(center - test_radius, center + test_radius)\n",
    "        )\n",
    "        phi0s = np.column_stack((x_coords.flatten(), y_coords.flatten()))\n",
    "    \n",
    "    # Profiling\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()\n",
    "    \n",
    "    a, b = vip_paco.PACOCalc(phi0s, use_subpixel_psf_astrometry=False, cpu=1)\n",
    "    \n",
    "    profiler.disable()\n",
    "    \n",
    "    # Convertir a tabla\n",
    "    s = StringIO()\n",
    "    ps = pstats.Stats(profiler, stream=s)\n",
    "    ps.sort_stats('cumulative')\n",
    "    \n",
    "    # Extraer datos para tabla\n",
    "    stats_data = []\n",
    "    for func_name, (cc, nc, tt, ct, callers) in ps.stats.items():\n",
    "        stats_data.append({\n",
    "            'Función': f\"{func_name[0]}:{func_name[1]}({func_name[2]})\",\n",
    "            'Llamadas': nc,\n",
    "            'Tiempo Total (s)': ct,\n",
    "            'Tiempo por Llamada (s)': ct/nc if nc > 0 else 0,\n",
    "            'Tiempo Acumulado (s)': tt\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(stats_data)\n",
    "    df = df.sort_values('Tiempo Total (s)', ascending=False)\n",
    "    \n",
    "    return a, b, df, profiler\n",
    "\n",
    "print(\"[OK] Funciones definidas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración\n",
    "REPO_URL = \"https://github.com/Waofin/tesis\"\n",
    "\n",
    "# Función para detectar automáticamente los datasets disponibles\n",
    "def detect_available_datasets(repo_path):\n",
    "    \"\"\"\n",
    "    Detecta automáticamente qué datasets están disponibles en el repositorio.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        Lista de tuplas (instrument, dataset_id) disponibles\n",
    "    \"\"\"\n",
    "    available = []\n",
    "    instruments = ['sphere_irdis', 'nirc2', 'lmircam']\n",
    "    \n",
    "    # Verificar que el directorio existe\n",
    "    if not repo_path.exists():\n",
    "        print(f\"[WARNING] Directorio no encontrado: {repo_path}\")\n",
    "        return available\n",
    "    \n",
    "    # Buscar archivos cube para cada instrumento y dataset_id\n",
    "    for instrument in instruments:\n",
    "        for dataset_id in range(1, 10):  # Buscar hasta dataset_id 9\n",
    "            cube_file = repo_path / f\"{instrument}_cube_{dataset_id}.fits\"\n",
    "            pa_file = repo_path / f\"{instrument}_pa_{dataset_id}.fits\"\n",
    "            psf_file = repo_path / f\"{instrument}_psf_{dataset_id}.fits\"\n",
    "            pxscale_file = repo_path / f\"{instrument}_pxscale_{dataset_id}.fits\"\n",
    "            \n",
    "            # Verificar que todos los archivos necesarios existan\n",
    "            if all(f.exists() for f in [cube_file, pa_file, psf_file, pxscale_file]):\n",
    "                available.append((instrument, dataset_id))\n",
    "    \n",
    "    return available\n",
    "\n",
    "# Detectar datasets disponibles automáticamente\n",
    "print(\"=\"*70)\n",
    "print(\"DETECTANDO DATASETS DISPONIBLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if 'TESIS_DIR' in globals():\n",
    "    repo_path = TESIS_DIR / \"subchallenge1\"\n",
    "    available_datasets = detect_available_datasets(repo_path)\n",
    "    \n",
    "    if available_datasets:\n",
    "        print(f\"\\n[OK] Se encontraron {len(available_datasets)} datasets disponibles:\")\n",
    "        for instrument, dataset_id in available_datasets:\n",
    "            print(f\"  - {instrument}_{dataset_id}\")\n",
    "    else:\n",
    "        print(\"\\n[WARNING] No se encontraron datasets. Usando lista por defecto.\")\n",
    "        available_datasets = [\n",
    "            ('sphere_irdis', 1),\n",
    "            ('sphere_irdis', 2),\n",
    "            ('sphere_irdis', 3),\n",
    "        ]\n",
    "else:\n",
    "    print(\"\\n[WARNING] TESIS_DIR no está definido. Usando lista por defecto.\")\n",
    "    available_datasets = [\n",
    "        ('sphere_irdis', 1),\n",
    "        ('sphere_irdis', 2),\n",
    "        ('sphere_irdis', 3),\n",
    "    ]\n",
    "\n",
    "# Lista de datasets a probar\n",
    "# Puedes modificar esta lista manualmente o usar la detección automática\n",
    "datasets_to_test = available_datasets\n",
    "\n",
    "# O descomenta y personaliza manualmente:\n",
    "# datasets_to_test = [\n",
    "#     ('sphere_irdis', 1),\n",
    "#     ('sphere_irdis', 2),\n",
    "#     ('sphere_irdis', 3),\n",
    "#     ('nirc2', 1),\n",
    "#     ('nirc2', 2),\n",
    "#     ('nirc2', 3),\n",
    "#     ('lmircam', 1),\n",
    "#     ('lmircam', 2),\n",
    "#     ('lmircam', 3),\n",
    "# ]\n",
    "\n",
    "print(f\"\\n[OK] Se procesarán {len(datasets_to_test)} datasets\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Almacenar resultados\n",
    "all_results = {}\n",
    "\n",
    "for instrument, dataset_id in datasets_to_test:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"PROCESANDO: {instrument}_{dataset_id}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    try:\n",
    "        # Cargar dataset\n",
    "        cube, angles, psf, pixscale = load_challenge_dataset_simple(\n",
    "            repo_url=REPO_URL,\n",
    "            instrument=instrument,\n",
    "            dataset_id=dataset_id,\n",
    "            branch='main'\n",
    "        )\n",
    "        \n",
    "        # Ejecutar benchmark\n",
    "        fwhm_arcsec = 4.0 * pixscale\n",
    "        results = benchmark_paco_simple(\n",
    "            cube, angles, psf, pixscale, fwhm_arcsec,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results['dataset'] = f\"{instrument}_{dataset_id}\"\n",
    "        results['cube_shape'] = cube.shape\n",
    "        all_results[f\"{instrument}_{dataset_id}\"] = results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n[ERROR] Error procesando {instrument}_{dataset_id}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        all_results[f\"{instrument}_{dataset_id}\"] = {\n",
    "            'success': False,\n",
    "            'errors': [str(e)]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualización Científica de Resultados\n",
    "\n",
    "Visualizaciones similares al notebook DC1_starting_kit: SNR maps, detecciones, y análisis científico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaciones científicas mejoradas\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "import pandas as pd\n",
    "\n",
    "def create_scientific_visualizations(results_dict, dataset_name):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones científicas similares al DC1_starting_kit\n",
    "    \"\"\"\n",
    "    if dataset_name not in results_dict or not results_dict[dataset_name].get('success', False):\n",
    "        print(f\"No hay resultados exitosos para {dataset_name}\")\n",
    "        return\n",
    "    \n",
    "    results = results_dict[dataset_name]\n",
    "    \n",
    "    # Obtener datos\n",
    "    snr = results['snr']\n",
    "    n_pixels = results['n_pixels']\n",
    "    \n",
    "    # Reshape para visualización\n",
    "    side = int(np.sqrt(n_pixels))\n",
    "    if side * side == n_pixels:\n",
    "        snr_2d = snr.reshape(side, side)\n",
    "    else:\n",
    "        snr_2d = snr[:side*side].reshape(side, side)\n",
    "    \n",
    "    # Crear figura con múltiples subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # 1. SNR Map (similar a DC1)\n",
    "    ax1 = plt.subplot(2, 3, 1)\n",
    "    im1 = ax1.imshow(snr_2d, origin='lower', cmap='hot', aspect='auto')\n",
    "    ax1.set_title(f'SNR Map - {dataset_name}\\nMax SNR: {results[\"snr_max\"]:.2f}', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('X (pixels)')\n",
    "    ax1.set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im1, ax=ax1, label='SNR')\n",
    "    \n",
    "    # 2. SNR Map con escala logarítmica\n",
    "    ax2 = plt.subplot(2, 3, 2)\n",
    "    snr_log = np.log10(np.abs(snr_2d) + 1e-10)\n",
    "    im2 = ax2.imshow(snr_log, origin='lower', cmap='viridis', aspect='auto')\n",
    "    ax2.set_title(f'SNR Map (log scale)\\nMean SNR: {results[\"snr_mean\"]:.2f}', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('X (pixels)')\n",
    "    ax2.set_ylabel('Y (pixels)')\n",
    "    plt.colorbar(im2, ax=ax2, label='log10(SNR)')\n",
    "    \n",
    "    # 3. Histograma de SNR\n",
    "    ax3 = plt.subplot(2, 3, 3)\n",
    "    valid_snr = snr[~np.isnan(snr) & (snr > 0)]\n",
    "    ax3.hist(valid_snr, bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax3.axvline(results['snr_max'], color='r', linestyle='--', linewidth=2, label=f'Max: {results[\"snr_max\"]:.2f}')\n",
    "    ax3.axvline(results['snr_mean'], color='g', linestyle='--', linewidth=2, label=f'Mean: {results[\"snr_mean\"]:.2f}')\n",
    "    ax3.set_xlabel('SNR')\n",
    "    ax3.set_ylabel('Frecuencia')\n",
    "    ax3.set_title('Distribución de SNR', fontsize=12, fontweight='bold')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Mapa de detecciones (thresholding)\n",
    "    ax4 = plt.subplot(2, 3, 4)\n",
    "    thresholds = [3, 5, 7]  # Diferentes umbrales de detección\n",
    "    colors = ['yellow', 'orange', 'red']\n",
    "    im4 = ax4.imshow(snr_2d, origin='lower', cmap='gray', aspect='auto', vmin=0, vmax=results['snr_max'])\n",
    "    \n",
    "    for thresh, color in zip(thresholds, colors):\n",
    "        mask = snr_2d > thresh\n",
    "        y_coords, x_coords = np.where(mask)\n",
    "        ax4.scatter(x_coords, y_coords, c=color, s=10, alpha=0.6, label=f'SNR > {thresh}')\n",
    "    \n",
    "    ax4.set_title('Detecciones por Umbral', fontsize=12, fontweight='bold')\n",
    "    ax4.set_xlabel('X (pixels)')\n",
    "    ax4.set_ylabel('Y (pixels)')\n",
    "    ax4.legend()\n",
    "    plt.colorbar(im4, ax=ax4, label='SNR')\n",
    "    \n",
    "    # 5. Estadísticas del dataset\n",
    "    ax5 = plt.subplot(2, 3, 5)\n",
    "    ax5.axis('off')\n",
    "    stats_text = f\"\"\"\n",
    "    DATASET: {dataset_name}\n",
    "    {'='*40}\n",
    "    \n",
    "    Cube Shape: {results.get('cube_shape', 'N/A')}\n",
    "    Píxeles Procesados: {results['n_pixels']}\n",
    "    Tiempo de Ejecución: {results['time']:.2f} s\n",
    "    \n",
    "    ESTADÍSTICAS SNR:\n",
    "    {'-'*40}\n",
    "    Máximo: {results['snr_max']:.2f}\n",
    "    Medio: {results['snr_mean']:.2f}\n",
    "    Desviación Estándar: {results.get('snr_std', 0):.2f}\n",
    "    Mediana: {np.nanmedian(snr):.2f}\n",
    "    \n",
    "    DETECCIONES:\n",
    "    {'-'*40}\n",
    "    SNR > 3σ: {np.sum(snr > 3)}\n",
    "    SNR > 5σ: {np.sum(snr > 5)}\n",
    "    SNR > 7σ: {np.sum(snr > 7)}\n",
    "    \"\"\"\n",
    "    ax5.text(0.1, 0.5, stats_text, fontsize=10, family='monospace',\n",
    "            verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 6. Comparación de rendimiento (si hay múltiples datasets)\n",
    "    ax6 = plt.subplot(2, 3, 6)\n",
    "    if len(results_dict) > 1:\n",
    "        datasets = [k for k, v in results_dict.items() if v.get('success', False)]\n",
    "        times = [results_dict[k]['time'] for k in datasets]\n",
    "        snr_maxs = [results_dict[k]['snr_max'] for k in datasets]\n",
    "        \n",
    "        ax6_twin = ax6.twinx()\n",
    "        bars1 = ax6.bar([x - 0.2 for x in range(len(datasets))], times, 0.4, \n",
    "                       label='Tiempo (s)', color='skyblue', alpha=0.7)\n",
    "        bars2 = ax6_twin.bar([x + 0.2 for x in range(len(datasets))], snr_maxs, 0.4,\n",
    "                            label='SNR Max', color='coral', alpha=0.7)\n",
    "        \n",
    "        ax6.set_xlabel('Dataset')\n",
    "        ax6.set_ylabel('Tiempo (s)', color='skyblue')\n",
    "        ax6_twin.set_ylabel('SNR Máximo', color='coral')\n",
    "        ax6.set_xticks(range(len(datasets)))\n",
    "        ax6.set_xticklabels([d.replace('_', '\\\\n') for d in datasets], rotation=0, ha='center')\n",
    "        ax6.set_title('Comparación de Rendimiento', fontsize=12, fontweight='bold')\n",
    "        ax6.legend(loc='upper left')\n",
    "        ax6_twin.legend(loc='upper right')\n",
    "        ax6.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        ax6.text(0.5, 0.5, 'Ejecuta múltiples\\ndatasets para comparar', \n",
    "                ha='center', va='center', fontsize=12, \n",
    "                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "        ax6.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Crear visualizaciones para cada dataset exitoso\n",
    "for dataset_name in all_results.keys():\n",
    "    if all_results[dataset_name].get('success', False):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"VISUALIZACIONES CIENTÍFICAS: {dataset_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        create_scientific_visualizations(all_results, dataset_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar photutils si no está disponible (necesario para detect_sources como en el starting kit)\n",
    "try:\n",
    "    from photutils.segmentation import detect_sources\n",
    "    print(\"[OK] photutils disponible\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] photutils no encontrado, instalando...\")\n",
    "    !pip install -q photutils\n",
    "    try:\n",
    "        from photutils.segmentation import detect_sources\n",
    "        print(\"[OK] photutils instalado\")\n",
    "    except ImportError:\n",
    "        # Fallback: intentar importación alternativa\n",
    "        try:\n",
    "            from photutils.segmentation import detect_sources\n",
    "            print(\"[OK] photutils instalado (importación alternativa)\")\n",
    "        except ImportError:\n",
    "            print(\"[ERROR] No se pudo importar detect_sources de photutils\")\n",
    "            print(\"  Intentando con scipy.ndimage como alternativa...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnóstico: ¿Por qué no hay detecciones?\n",
    "\n",
    "Según la documentación del EIDC, cada dataset tiene **0-5 señales planetarias sintéticas inyectadas**. \n",
    "\n",
    "**Razones posibles para no tener detecciones:**\n",
    "\n",
    "1. **El dataset puede tener 0 señales inyectadas** (es válido según EIDC)\n",
    "2. **SNR máximo bajo**: Si el SNR máximo está por debajo de 3σ, no habrá detecciones\n",
    "3. **Parámetros de PACO**: Los parámetros (fwhm, patch_size) pueden necesitar ajuste\n",
    "4. **Regiones no procesadas**: Las señales pueden estar en regiones con máscara interna\n",
    "5. **Sobre-substracción**: El algoritmo puede estar suprimiendo señales reales\n",
    "\n",
    "**Nota**: La función de validación cuenta **grupos de píxeles conectados** (clustering espacial), no píxeles individuales, para evitar falsos positivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Profiling Detallado con Tabla de Tiempos por Función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling detallado con tabla profesional\n",
    "print(\"=\"*70)\n",
    "print(\"PROFILING DETALLADO DEL ALGORITMO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Seleccionar el primer dataset exitoso para profiling\n",
    "profile_dataset = None\n",
    "for dataset_name, results in all_results.items():\n",
    "    if results.get('success', False):\n",
    "        profile_dataset = dataset_name\n",
    "        break\n",
    "\n",
    "if profile_dataset:\n",
    "    # Manejar nombres con múltiples partes (ej: 'sphere_irdis_1' -> instrument='sphere_irdis', dataset_id='1')\n",
    "    parts = profile_dataset.split('_')\n",
    "    instrument = '_'.join(parts[:-1])  # Todo excepto el último\n",
    "    dataset_id = int(parts[-1])  # El último es el ID\n",
    "    \n",
    "    print(f\"\\nEjecutando profiling para: {profile_dataset}\")\n",
    "    print(\"(Esto puede tardar unos minutos...)\\n\")\n",
    "    \n",
    "    # Recargar datos\n",
    "    cube, angles, psf, pixscale = load_challenge_dataset_simple(\n",
    "        repo_url=REPO_URL,\n",
    "        instrument=instrument,\n",
    "        dataset_id=dataset_id,\n",
    "        branch='main'\n",
    "    )\n",
    "    \n",
    "    fwhm_arcsec = 4.0 * pixscale\n",
    "    \n",
    "    # Ejecutar profiling\n",
    "    try:\n",
    "        a_prof, b_prof, profile_df, profiler = profile_paco_detailed(\n",
    "            cube, angles, psf, pixscale, fwhm_arcsec\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TABLA DE TIEMPOS POR FUNCIÓN (Top 20)\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Mostrar tabla formateada\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        pd.set_option('display.max_columns', None)\n",
    "        pd.set_option('display.width', None)\n",
    "        pd.set_option('display.max_colwidth', 50)\n",
    "        \n",
    "        # Formatear números\n",
    "        display_df = profile_df.head(20).copy()\n",
    "        display_df['Tiempo Total (s)'] = display_df['Tiempo Total (s)'].apply(lambda x: f\"{x:.4f}\")\n",
    "        display_df['Tiempo por Llamada (s)'] = display_df['Tiempo por Llamada (s)'].apply(lambda x: f\"{x:.6f}\")\n",
    "        display_df['Tiempo Acumulado (s)'] = display_df['Tiempo Acumulado (s)'].apply(lambda x: f\"{x:.4f}\")\n",
    "        \n",
    "        print(display_df.to_string(index=False))\n",
    "        \n",
    "        # Crear visualización de la tabla\n",
    "        fig, ax = plt.subplots(figsize=(14, 10))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Preparar datos para la tabla\n",
    "        table_data = profile_df.head(15)[['Función', 'Llamadas', 'Tiempo Total (s)', 'Tiempo por Llamada (s)']].copy()\n",
    "        table_data['Tiempo Total (s)'] = table_data['Tiempo Total (s)'].apply(lambda x: f\"{x:.4f}\")\n",
    "        table_data['Tiempo por Llamada (s)'] = table_data['Tiempo por Llamada (s)'].apply(lambda x: f\"{x:.6f}\")\n",
    "        \n",
    "        table = ax.table(cellText=table_data.values,\n",
    "                        colLabels=['Función', 'Llamadas', 'Tiempo Total (s)', 'Tiempo/Llamada (s)'],\n",
    "                        cellLoc='left',\n",
    "                        loc='center',\n",
    "                        colWidths=[0.5, 0.1, 0.15, 0.15])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1, 2)\n",
    "        \n",
    "        # Colorear filas alternas\n",
    "        for i in range(1, len(table_data) + 1):\n",
    "            if i % 2 == 0:\n",
    "                for j in range(4):\n",
    "                    table[(i, j)].set_facecolor('#f0f0f0')\n",
    "        \n",
    "        plt.title(f'Profiling Detallado - {profile_dataset}\\nTop 15 Funciones por Tiempo Total', \n",
    "                 fontsize=14, fontweight='bold', pad=20)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Guardar perfil completo\n",
    "        print(f\"\\n[OK] Profiling completado\")\n",
    "        print(f\"  Total de funciones analizadas: {len(profile_df)}\")\n",
    "        print(f\"  Tiempo total: {profile_df['Tiempo Total (s)'].sum():.4f} segundos\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Error en profiling: {e}\")\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"No hay datasets exitosos para profiling\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validaciones Científicas (Detección y Análisis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validaciones científicas similares a DC1_starting_kit\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDACIONES CIENTÍFICAS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def scientific_validation(results_dict, dataset_name, fwhm_pixels=4.0):\n",
    "    \"\"\"\n",
    "    Realiza validaciones científicas: detecciones, umbrales, análisis de calidad\n",
    "    \"\"\"\n",
    "    if dataset_name not in results_dict or not results_dict[dataset_name].get('success', False):\n",
    "        return None\n",
    "    \n",
    "    results = results_dict[dataset_name]\n",
    "    snr = results['snr']\n",
    "    n_pixels = results['n_pixels']\n",
    "    \n",
    "    # Reshape\n",
    "    side = int(np.sqrt(n_pixels))\n",
    "    if side * side == n_pixels:\n",
    "        snr_2d = snr.reshape(side, side)\n",
    "    else:\n",
    "        snr_2d = snr[:side*side].reshape(side, side)\n",
    "    \n",
    "    # Calcular FWHM en píxeles (aproximado)\n",
    "    fwhm = fwhm_pixels\n",
    "    \n",
    "    # Análisis de detecciones con diferentes umbrales\n",
    "    thresholds = [3, 4, 5, 6, 7]\n",
    "    detections = {}\n",
    "    \n",
    "    # DIAGNÓSTICO: Mostrar estadísticas de SNR antes de calcular detecciones\n",
    "    print(f\"\\n[DIAGNÓSTICO] Estadísticas de SNR para {dataset_name}:\")\n",
    "    print(f\"  SNR máximo: {results['snr_max']:.2f}\")\n",
    "    print(f\"  SNR medio: {results['snr_mean']:.2f}\")\n",
    "    print(f\"  SNR mediana: {np.nanmedian(snr):.2f}\")\n",
    "    print(f\"  Desv. estándar: {np.nanstd(snr):.2f}\")\n",
    "    print(f\"  Píxeles con SNR > 3: {np.sum(snr_2d > 3)}\")\n",
    "    print(f\"  Píxeles con SNR > 5: {np.sum(snr_2d > 5)}\")\n",
    "    print(f\"  Píxeles con SNR > 7: {np.sum(snr_2d > 7)}\")\n",
    "    \n",
    "    # Intentar usar photutils.detect_sources (método del starting kit)\n",
    "    try:\n",
    "        from photutils.segmentation import detect_sources\n",
    "        \n",
    "        for thresh in thresholds:\n",
    "            # Usar detect_sources para agrupar píxeles conectados (blobs)\n",
    "            # npix=2: mínimo de píxeles conectados para considerar una detección (como en starting kit)\n",
    "            # connectivity=4: conectividad de 4 vecinos\n",
    "            segments = detect_sources(snr_2d, thresh, npix=2, connectivity=4)\n",
    "            \n",
    "            if segments is None:\n",
    "                n_detections = 0\n",
    "                n_pixels = 0\n",
    "                detected_snr = []\n",
    "            else:\n",
    "                # Contar el número de blobs (detecciones) únicos\n",
    "                n_detections = segments.nlabels\n",
    "                # Contar píxeles totales en todos los blobs\n",
    "                n_pixels = np.sum(segments.data != 0)\n",
    "                # Obtener SNR de los píxeles detectados\n",
    "                detected_snr = snr_2d[segments.data != 0]\n",
    "            \n",
    "            if n_detections > 0 and len(detected_snr) > 0:\n",
    "                # Calcular SNR máximo por blob\n",
    "                max_snr_per_blob = []\n",
    "                for label_id in range(1, n_detections + 1):\n",
    "                    blob_mask = segments.data == label_id\n",
    "                    max_snr_per_blob.append(np.nanmax(snr_2d[blob_mask]))\n",
    "                \n",
    "                detections[thresh] = {\n",
    "                    'count': n_detections,  # Número de blobs (detecciones reales)\n",
    "                    'n_pixels': n_pixels,  # Número total de píxeles en blobs\n",
    "                    'mean_snr': float(np.nanmean(detected_snr)),\n",
    "                    'max_snr': float(np.nanmax(detected_snr)),\n",
    "                    'mean_max_snr_per_blob': float(np.nanmean(max_snr_per_blob)),\n",
    "                    'std_snr': float(np.nanstd(detected_snr))\n",
    "                }\n",
    "            else:\n",
    "                detections[thresh] = {\n",
    "                    'count': 0,\n",
    "                    'n_pixels': 0,\n",
    "                    'mean_snr': 0,\n",
    "                    'max_snr': 0,\n",
    "                    'mean_max_snr_per_blob': 0,\n",
    "                    'std_snr': 0\n",
    "                }\n",
    "    except ImportError:\n",
    "        print(\"[WARNING] photutils no disponible, usando scipy.ndimage como fallback\")\n",
    "        # Fallback: usar scipy.ndimage para clustering\n",
    "        try:\n",
    "            from scipy import ndimage\n",
    "            \n",
    "            for thresh in thresholds:\n",
    "                mask = snr_2d > thresh\n",
    "                n_pixels = np.sum(mask)\n",
    "                \n",
    "                if n_pixels > 0:\n",
    "                    labeled_mask, n_detections = ndimage.label(mask)\n",
    "                    detected_snr = snr_2d[mask]\n",
    "                    \n",
    "                    # Calcular SNR máximo por blob\n",
    "                    max_snr_per_blob = []\n",
    "                    for label_id in range(1, n_detections + 1):\n",
    "                        blob_mask = labeled_mask == label_id\n",
    "                        max_snr_per_blob.append(np.nanmax(snr_2d[blob_mask]))\n",
    "                    \n",
    "                    detections[thresh] = {\n",
    "                        'count': n_detections,\n",
    "                        'n_pixels': n_pixels,\n",
    "                        'mean_snr': float(np.nanmean(detected_snr)),\n",
    "                        'max_snr': float(np.nanmax(detected_snr)),\n",
    "                        'mean_max_snr_per_blob': float(np.nanmean(max_snr_per_blob)) if max_snr_per_blob else 0,\n",
    "                        'std_snr': float(np.nanstd(detected_snr))\n",
    "                    }\n",
    "                else:\n",
    "                    detections[thresh] = {\n",
    "                        'count': 0,\n",
    "                        'n_pixels': 0,\n",
    "                        'mean_snr': 0,\n",
    "                        'max_snr': 0,\n",
    "                        'mean_max_snr_per_blob': 0,\n",
    "                        'std_snr': 0\n",
    "                    }\n",
    "        except ImportError:\n",
    "            print(\"[WARNING] scipy.ndimage no disponible, usando conteo simple de píxeles\")\n",
    "            # Último fallback: contar píxeles individuales\n",
    "            for thresh in thresholds:\n",
    "                mask = snr_2d > thresh\n",
    "                n_pixels = np.sum(mask)\n",
    "                \n",
    "                if n_pixels > 0:\n",
    "                    detected_snr = snr_2d[mask]\n",
    "                    detections[thresh] = {\n",
    "                        'count': n_pixels,  # En este caso, cada píxel cuenta como detección\n",
    "                        'n_pixels': n_pixels,\n",
    "                        'mean_snr': float(np.nanmean(detected_snr)),\n",
    "                        'max_snr': float(np.nanmax(detected_snr)),\n",
    "                        'std_snr': float(np.nanstd(detected_snr))\n",
    "                    }\n",
    "                else:\n",
    "                    detections[thresh] = {\n",
    "                        'count': 0,\n",
    "                        'n_pixels': 0,\n",
    "                        'mean_snr': 0,\n",
    "                        'max_snr': 0,\n",
    "                        'std_snr': 0\n",
    "                    }\n",
    "    \n",
    "    # Información sobre por qué puede no haber detecciones\n",
    "    if all(detections[t]['count'] == 0 for t in [3, 5, 7]):\n",
    "        print(f\"\\n[ANÁLISIS] Razones posibles para no tener detecciones:\")\n",
    "        print(f\"  1. El dataset puede tener 0 señales inyectadas (según EIDC: 0-5 señales por dataset)\")\n",
    "        print(f\"  2. SNR máximo ({results['snr_max']:.2f}) puede estar por debajo de los umbrales (3-7σ)\")\n",
    "        if results['snr_max'] < 3:\n",
    "            print(f\"     → SNR máximo < 3σ: Las señales pueden ser demasiado débiles\")\n",
    "        print(f\"  3. Las señales pueden estar en regiones no procesadas (máscara interna)\")\n",
    "        print(f\"  4. Los parámetros de PACO pueden necesitar ajuste (fwhm, patch_size, etc.)\")\n",
    "        print(f\"  5. El algoritmo puede estar suprimiendo señales reales (sobre-substracción)\")\n",
    "        print(f\"\\n  NOTA: El método ahora agrupa píxeles conectados en 'blobs' (como el starting kit),\")\n",
    "        print(f\"        por lo que cuenta detecciones reales, no píxeles individuales.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Resumen Final y Exportación de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen completo y exportar resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN FINAL Y EXPORTACIÓN\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Crear DataFrame con todos los resultados\n",
    "summary_data = []\n",
    "for dataset_name, results in all_results.items():\n",
    "    if results.get('success', False):\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Exito': '[OK]',\n",
    "            'Tiempo (s)': f\"{results.get('time', 0):.2f}\",\n",
    "            'Píxeles': results.get('n_pixels', 0),\n",
    "            'SNR Máximo': f\"{results.get('snr_max', 0):.2f}\",\n",
    "            'SNR Medio': f\"{results.get('snr_mean', 0):.2f}\",\n",
    "            'Cube Shape': str(results.get('cube_shape', 'N/A'))\n",
    "        })\n",
    "    else:\n",
    "        summary_data.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Exito': '[ERROR]',\n",
    "            'Tiempo (s)': 'N/A',\n",
    "            'Píxeles': 'N/A',\n",
    "            'SNR Máximo': 'N/A',\n",
    "            'SNR Medio': 'N/A',\n",
    "            'Cube Shape': 'N/A'\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "# Mostrar tabla de resumen\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TABLA RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "# Crear visualización de la tabla\n",
    "fig, ax = plt.subplots(figsize=(14, max(6, len(summary_data) * 0.5)))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=summary_df.values,\n",
    "                colLabels=summary_df.columns,\n",
    "                cellLoc='center',\n",
    "                loc='center',\n",
    "                colWidths=[0.15, 0.08, 0.12, 0.12, 0.12, 0.12, 0.3])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1, 2.5)\n",
    "\n",
    "# Colorear filas según éxito\n",
    "for i in range(len(summary_data)):\n",
    "    if summary_data[i]['Exito'] == '[OK]':\n",
    "        for j in range(len(summary_df.columns)):\n",
    "            table[(i+1, j)].set_facecolor('#d4edda')\n",
    "    else:\n",
    "        for j in range(len(summary_df.columns)):\n",
    "            table[(i+1, j)].set_facecolor('#f8d7da')\n",
    "\n",
    "plt.title('Resumen Completo de Benchmarks', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Exportar resultados a CSV\n",
    "try:\n",
    "    summary_df.to_csv('/content/benchmark_results_summary.csv', index=False)\n",
    "    print(\"\\n[OK] Resultados exportados a: /content/benchmark_results_summary.csv\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[WARNING] No se pudo exportar CSV: {e}\")\n",
    "\n",
    "# Resumen final\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN FINAL\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total de datasets procesados: {len(all_results)}\")\n",
    "print(f\"Datasets exitosos: {len([r for r in all_results.values() if r.get('success', False)])}\")\n",
    "print(f\"Tiempo total de ejecución: {sum([r.get('time', 0) for r in all_results.values() if r.get('success', False)]):.2f} segundos\")\n",
    "\n",
    "if validation_results:\n",
    "    print(f\"\\nValidaciones científicas completadas: {len(validation_results)}\")\n",
    "    for dataset_name, detections in validation_results.items():\n",
    "        print(f\"\\n  {dataset_name}:\")\n",
    "        for thresh in [3, 5, 7]:\n",
    "            if thresh in detections:\n",
    "                print(f\"    - Detecciones SNR > {thresh}σ: {detections[thresh]['count']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIN DEL BENCHMARK Y ANÁLISIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPara más información sobre los errores encontrados, consulta:\")\n",
    "print(\"  - ANALISIS_ERRORES_PACO_VIP.md\")\n",
    "print(\"  - benchmark_paco_vip.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualización de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones\n",
    "n_datasets = len([r for r in all_results.values() if r.get('success', False)])\n",
    "\n",
    "if n_datasets > 0:\n",
    "    fig, axes = plt.subplots(1, n_datasets, figsize=(6*n_datasets, 5))\n",
    "    if n_datasets == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for dataset_name, results in all_results.items():\n",
    "        if results.get('success', False) and 'snr' in results:\n",
    "            snr = results['snr']\n",
    "            n_pixels = results['n_pixels']\n",
    "            \n",
    "            # Reshape para visualización\n",
    "            side = int(np.sqrt(n_pixels))\n",
    "            if side * side == n_pixels:\n",
    "                snr_2d = snr.reshape(side, side)\n",
    "            else:\n",
    "                snr_2d = snr[:side*side].reshape(side, side)\n",
    "            \n",
    "            im = axes[plot_idx].imshow(snr_2d, origin='lower', cmap='hot')\n",
    "            axes[plot_idx].set_title(f'{dataset_name}\\n({results[\"time\"]:.2f}s, SNR max: {results[\"snr_max\"]:.2f})')\n",
    "            axes[plot_idx].set_xlabel('X (pixels)')\n",
    "            axes[plot_idx].set_ylabel('Y (pixels)')\n",
    "            plt.colorbar(im, ax=axes[plot_idx], label='SNR')\n",
    "            plot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay resultados exitosos para visualizar\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Resumen de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar resumen\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset_name, results in all_results.items():\n",
    "    print(f\"\\nDataset: {dataset_name}\")\n",
    "    print(f\"  Exito: {'[OK]' if results.get('success', False) else '[ERROR]'}\")\n",
    "    \n",
    "    if results.get('success', False):\n",
    "        print(f\"  Tiempo: {results.get('time', 0):.2f} segundos\")\n",
    "        print(f\"  Píxeles procesados: {results.get('n_pixels', 0)}\")\n",
    "        print(f\"  SNR máximo: {results.get('snr_max', 0):.2f}\")\n",
    "        print(f\"  SNR medio: {results.get('snr_mean', 0):.2f}\")\n",
    "        if 'cube_shape' in results:\n",
    "            print(f\"  Cube shape: {results['cube_shape']}\")\n",
    "    else:\n",
    "        print(f\"  Errores:\")\n",
    "        for error in results.get('errors', []):\n",
    "            print(f\"    - {error}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FIN DEL BENCHMARK\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nPara más información sobre los errores encontrados, consulta:\")\n",
    "print(\"  - ANALISIS_ERRORES_PACO_VIP.md\")\n",
    "print(\"  - benchmark_paco_vip.py\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
